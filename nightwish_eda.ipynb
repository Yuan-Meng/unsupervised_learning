{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Background\n",
    "\n",
    "[Nightwish](https://nightwish.com/) is a symphonic power metal band from Finland and one of the biggest names in the European metal scene. Since 1996, they have released 9 studio albums and numerous singles, EP's, and live albums. The most recent album *Human. :II: Nature.* just came out in April 2020. Throughout their career, Nightwish has explored a wide variety of themes, ranging from personal topics to science and nature. As a Nightwish fan and aspiring data scientist, I thought it might be interesting to look at how the lyrical themes of Nightwish have evolved over the past 24 years.\n",
    "\n",
    "Inpsired by the [Taylor Swift dataset](https://www.kaggle.com/PromptCloudHQ/taylor-swift-song-lyrics-from-all-the-albums) on Kaggle ([example analysis](https://news.codecademy.com/taylor-swift-lyrics-machine-learning/)), I scraped all of Nightwish's lyrics from the metal lyrics archive [Dark Lyrics](http://www.darklyrics.com/) [(script)](https://github.com/medakk/darklyrics-scraper) and shared [my dataset](https://www.kaggle.com/crazyrichbayesians/nightwish-lyrics) on Kaggle. Eventually, I want to use topic modeling to find out the most common topics in Nightwish songs and examine how they have changed over time. Even cooler, it may be possible to train a neural net to write new lyrics in the style of Nightwish!\n",
    "\n",
    "Before all that fancy modeling, I'll do some exploratory data analysis (EDA) first. For instance, how do lyrics cluster together? How can we represent them in a low-dimensional space?... This notebook is dedicated to EDA of the Nightwish lyrics dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import en_core_web_lg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import umap\n",
    "from gensim.models import word2vec\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import clean\n",
    "import word_2dviz\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from text file\n",
    "with open(\"nightwish_lyrics.txt\") as f:\n",
    "    raw = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of unwanted strings\n",
    "raw = [\n",
    "    re.sub(r\"\\*{2,}\", \"\", str(i)) for i in raw\n",
    "]  # Dividers '****************************'\n",
    "\n",
    "raw = [i for i in raw if i]  # Empty strings\n",
    "\n",
    "raw = [i for i in raw if \"[\" not in i]  # Strings containing \"[\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "df = pd.DataFrame(raw, columns=[\"lyric\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Text mining\n",
    "With all information cramped into one column, the information in the original dataset is hard to use. The goal of text mining is to extract album titles, years, track titles, track numbers, and lyrics from the original data and create a separate column for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting titles\n",
    "def title_extract(idx_start):\n",
    "\n",
    "    # Where album/song ends\n",
    "    idx_end = idx_start[1:]\n",
    "\n",
    "    idx_end.append(len(df))\n",
    "\n",
    "    # Number of lines in album/song\n",
    "    lines = [i - j for i, j in zip(idx_end, idx_start)]\n",
    "\n",
    "    # Album/song titles\n",
    "    titles = df[\"lyric\"].iloc[idx_start].values.tolist()\n",
    "\n",
    "    # Repeat titles by the number of lines\n",
    "    titles_nested = [[i] * j for i, j in zip(titles, lines)]\n",
    "\n",
    "    # Return as a list\n",
    "    return [item for items in titles_nested for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find beginning of ablum titles\n",
    "album_start = df.index[df[\"lyric\"].str.contains(\"*\", regex=False)].tolist()\n",
    "\n",
    "# Add album titles\n",
    "df[\"album\"] = title_extract(album_start)\n",
    "\n",
    "# Remove album rows\n",
    "df = df.drop(album_start).reset_index(drop=True)\n",
    "\n",
    "# Find beginning song titles\n",
    "song_start = df.index[df[\"lyric\"].str.contains(r\"^[0-9]{1,2}\\.\\s\", regex=True)].tolist()\n",
    "\n",
    "# Add song titles\n",
    "df[\"track\"] = title_extract(song_start)\n",
    "\n",
    "# Remove song rows\n",
    "df = df.drop(song_start).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract album titles\n",
    "df[\"album_title\"] = df[\"album\"].apply(\n",
    "    lambda x: (\n",
    "        re.findall(r'\"([^\"]*)\"', x)[0] if re.findall(r'\"([^\"]*)\"', x) else \"Non-Album\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Extract years\n",
    "df[\"year\"] = df[\"album\"].apply(\n",
    "    lambda x: (\n",
    "        re.findall(r\"\\(([^)]+)\", x)[0]\n",
    "        if re.findall(r'\"([^\"]*)\"', x)\n",
    "        else \"Unknown Year\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Extract track titles and numbers\n",
    "df[\"track_title\"] = df[\"track\"].apply(lambda x: str(x).split(\". \")[1])\n",
    "\n",
    "df[\"track_number\"] = df[\"track\"].apply(lambda x: str(x).split(\". \")[0])\n",
    "\n",
    "# Drop original ablum and track columns\n",
    "df.drop([\"album\", \"track\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "A powerful way to represent word meanings is [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). The input can be words, phrases, and sentences and the output is a vector that represents each word/phrase/sentence. The cool thing is that those close in meaning are also closer in the vector space. Several Python libraries can implement Word2Vec. Here I'm using the most popular two, spaCy and Gensim. Text cleaning comes first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning\n",
    "I created a custom module `clean`for text cleaning, which includes functions for normalization, stop word removal, and lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "First of all, we want the same words to be treated as the same. This means we need to deal with cases (e.g., \"Have\" vs. \"have\"), contradictions (e.g., \"it's\" vs. \"it is\"), misspellings (e.g., \"believ\" vs. \"believe\"), etc. It's also a [standard practice](https://www.kaggle.com/saga21/disaster-tweets-comp-introduction-to-nlp/notebook) in Natural Language Processing (NLP) to replace accented characters ( e.g., \"Ã‰lan\") with \"plain\" characters (e.g., \"Elan\") and to remove punctuations, white spaces, etc. that don't bear any meanings. This whole process is often called \"normalization\".\n",
    "\n",
    "The `clean` module has 6 normalization functions: `lower_case()`, `expand_contractions()`, `remove_punct()`, `remove_accented_chars()`,  `correct_spellings()`, and `remove_space()`. The catch-all `normalization()` function executes them in exactly this order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "df[\"lyric_normalized\"] = df[\"lyric\"].apply(lambda x: clean.normalization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove new punctuations created by SpellChecker()\n",
    "df[\"lyric_normalized\"] = df[\"lyric_normalized\"].apply(lambda x: clean.remove_punct(x))\n",
    "\n",
    "# Remove new spaces\n",
    "df[\"lyric_normalized\"] = df[\"lyric_normalized\"].apply(lambda x: clean.remove_space(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop word removal\n",
    "[Stop words](https://kavita-ganesan.com/what-are-stop-words/#.Xp9ZE9NKjpA) are common words that don't often contribute new information, such as pronouns (e.g., I, its, yourself, etc.), determiners (e.g., a, the), conjunctions (e.g., for, an, nor, but, or, yet, so), and prepositions (e.g., in, under, towards, before). We can use the spaCy library to remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained English model\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "df[\"lyric_nostop\"] = df[\"lyric_normalized\"].apply(\n",
    "    lambda x: clean.remove_stopwords(nlp(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "Words like \"wanted\", \"wants\", and \"want\" share the same root but have different inflectional forms. Instead of representing each form separately, we can use \"lemmatization\" to return each word to its [lemma](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) (i.e., its base or dictionary form). The spaCy libarary also makes lemmatization really convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize words\n",
    "df[\"lyric_lemma\"] = df[\"lyric_nostop\"].apply(lambda x: clean.lemmatizer(nlp(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results\n",
    "We can save the cleaned dataset so in the future we don't have to go through the entire process from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "df.to_csv(\"nightwish_lyrics.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that no longer have lyrics\n",
    "df_clean = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "We can use Word2Vec to create word embeddings on different levels. For instance, we can create a vector for each line of lyrics or we can do it for each word in Nightwish's \"vocabulary\". I'm doing both below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "#### Each line of lyrics\n",
    "I used a pre-trained model in spaCy to create a vector for each line of lyrics, which is the average vector across all words in that line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average vectors for each line of lyrics\n",
    "X_lyric = np.array(\n",
    "    [nlp(lyric[\"lyric_lemma\"]).vector for idx, lyric in df_clean.iterrows()]\n",
    ")\n",
    "\n",
    "# Center the vectors\n",
    "vec_mean = X_lyric.mean(axis=0)\n",
    "\n",
    "X_lyrics = pd.DataFrame([vec - vec_mean for vec in X_lyric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each word in vocabulary\n",
    "Alternatively, we can put all the words Nightwish has ever used together in two a single corpus and create a vector for each word in this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus\n",
    "corpus = df_clean[\"lyric_lemma\"].tolist()\n",
    "\n",
    "corpus = [i for i in corpus if i]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [sentence.split() for sentence in corpus]\n",
    "\n",
    "# Train model (words should appear at least 10 times)\n",
    "model = word2vec.Word2Vec(tokenized_sentences, min_count=10)\n",
    "\n",
    "# Save model\n",
    "model.save(\"model.bin\")\n",
    "\n",
    "# Retrieve vectors\n",
    "X_vocab = model[model.wv.vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "#### Standardization\n",
    "Since methods such as PCA can be sensitive to scales, let's standardize the vectors before using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings of lyrics\n",
    "lyrics_scaler = StandardScaler()\n",
    "X_lyrics_std = lyrics_scaler.fit_transform(X_lyrics)\n",
    "\n",
    "# Embeddings of vocabulary\n",
    "vocab_scaler = StandardScaler()\n",
    "X_vocab_std = vocab_scaler.fit_transform(X_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster analysis\n",
    "Instead of using all columns in word embeddings, I'm going to extract two principle components and use them to train models. In the interest of space, I'll focus on vocabulary-level embeddings below. The first question is, **how many clusters does Nightwish's lyrical vocabulary have?** \n",
    "\n",
    "Since our data doesn't have labels, we can use the silhouette score to evaluate clustering results. This metric measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). Higher values indicate more appropriate clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 2 principle components\n",
    "X_vocab_pca = PCA(n_components=2).fit_transform(X_vocab_std)\n",
    "\n",
    "# Visualize on a 2D plane\n",
    "plt.scatter(X_vocab_pca[:, 0], X_vocab_pca[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means\n",
    "The simplest clustering algorithm is k-means. The hyper-paramter k determines the number of clusters to use. We can try k from 1 to 10 to see which value might be the optimal trade-off between complexity and model fit. I'll use the \"elbow method\" to find such a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method: Plot sse against k\n",
    "sse = []\n",
    "list_k = list(range(1, 10))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(X_vocab_pca)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, \"-o\")\n",
    "plt.xlabel(r\"Number of clusters *k*\")\n",
    "plt.ylabel(\"Sum of squared distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't an obvious elbow with a sharp decrease but it seems the sum of squared distance doesn't drop too much when k increases from 4. So let's set the number of clusters to be 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KMeans object using k=4\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "\n",
    "# Fit model\n",
    "y_kmeans = kmeans.fit_predict(X_vocab_pca)\n",
    "\n",
    "# Visualize results\n",
    "plt.scatter(X_vocab_pca[:, 0], X_vocab_pca[:, 1], c=y_kmeans, s=50, cmap=\"viridis\")\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c=\"black\", s=200, alpha=0.5)\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate results\n",
    "kmeans_silhouette = round(float(metrics.silhouette_score(X_vocab_pca, y_kmeans)), 2)\n",
    "print(f\"The silhouette score of k-means using 4 clusters is {kmeans_silhouette}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Hierarchical clustering\n",
    "Rather than specifying the number of clusters beforehand, we can let the algorithm \"discover\" clusters on the fly. Hierarchical clustering is such an algorithm. There are different ways of assigning observations to clusters, each called a different linkage method. We can try different methods to see which one works the best for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different linkage methods\n",
    "links = [\"average\", \"complete\", \"ward\"]\n",
    "\n",
    "for link in links:\n",
    "\n",
    "    # Create object using each method\n",
    "    hier_model = AgglomerativeClustering(linkage=link, affinity=\"euclidean\")\n",
    "\n",
    "    # Fit model\n",
    "    y_hier = hier_model.fit_predict(X_vocab_pca)\n",
    "\n",
    "    # Visualize results\n",
    "    plt.scatter(X_vocab_pca[:, 0], X_vocab_pca[:, 1], c=y_hier, s=50, cmap=\"viridis\")\n",
    "    plt.title(f\"Hierachical Clustering ({link.capitalize()})\")\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate results\n",
    "    hier_silhouette = round(float(metrics.silhouette_score(X_vocab_pca, y_hier)), 2)\n",
    "    print(\n",
    "        f\"The silhouette score of hierarchical clustering ({link}) is {hier_silhouette}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from silhouette scores, the average method works the best and is a huge improvement from k-means clustering results. According to this method, almost all observations belong to one big cluster, with just one exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "DBSCAN is another algorithm that discovers clusters as it goes. It starts with a random point and finds all the density-reachable points to cluster together. We can adjust the largest radius for expanding clusters ($\\epsilon$) and the minimum number of points needed to form a cluster (\"MinPts\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter tuning\n",
    "eps_list = np.linspace(0.1, 10, num=100, endpoint=False)\n",
    "min_samples_list = np.linspace(1, 30, num=30, endpoint=True)\n",
    "\n",
    "scores = {}\n",
    "for eps in eps_list:\n",
    "    for min_samples in min_samples_list:\n",
    "        dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        pred = dbscan_model.fit_predict(X_vocab_pca)\n",
    "        key = \"eps = {}, MinPts = {}\".format(round(eps, 2), round(min_samples, 1))\n",
    "        try:\n",
    "            scores[key] = metrics.silhouette_score(X_vocab_pca, pred)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Show results\n",
    "max_score = round(float(scores.get(max(scores, key=scores.get))), 2)\n",
    "\n",
    "print(f\"Highest Silhouette score was {max_score} when {max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with best model\n",
    "dbscan = DBSCAN(eps=1.98, min_samples=1)\n",
    "y_dbscan = dbscan.fit_predict(X_vocab_pca)\n",
    "\n",
    "# Visualize results\n",
    "plt.scatter(X_vocab_pca[:, 0], X_vocab_pca[:, 1], c=y_dbscan, s=50, cmap=\"viridis\")\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate results\n",
    "dbscan_silhouette = round(float(metrics.silhouette_score(X_vocab_pca, y_dbscan)), 2)\n",
    "print(f\"The silhouette score of DBSCAN using 2 clusters is {dbscan_silhouette}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN with the best-fitting hyper-parameters achieved similar results as hierarchical clustering using the Ward method. It also views all observations as belong to one cluster, with few in a different cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM\n",
    "All the clustering methods above assume that one observation belongs to only one cluster. This type of algorithm is so-called hard clustering algorithms. Alternatively, one observation can belong to any cluster with differing probabilities. This is soft clustering. A popular soft clustering algorithm is Gaussian Mixture Models (GMMs) where data are assumed to be generated by multiple Gaussian distributions. Let's try GMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model project\n",
    "gmm = GaussianMixture(n_components=2, random_state=123)\n",
    "\n",
    "# Predict clusters\n",
    "y_gmm = gmm.fit_predict(X_vocab_pca)\n",
    "\n",
    "# Visualize results\n",
    "plt.scatter(X_vocab_pca[:, 0], X_vocab_pca[:, 1], c=y_gmm, s=50, cmap=\"viridis\")\n",
    "plt.title(\"GMM Clustering\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate results\n",
    "gmm_silhouette = round(float(metrics.silhouette_score(X_vocab_pca, y_gmm)), 2)\n",
    "print(f\"The silhouette score of GMM using 2 clusters is {gmm_silhouette}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMM inferred two clusters. It didn't perform as well as hierarchical clustering or DBSCAN. So perhaps words in Nightwish's lyrics are indeed similar to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction\n",
    "Lastly, let's see which words are similar in space and which ones are far apart. Due to the high number of columns in our word embeddings, it's necessary to use dimension reduction techniques such as PCA, t-SNE, and UMAP. Like before, I created a custom module called \"word_2divz\" to do the job. To avoid overplotting, I only selected words that appeared at least 10 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension reduction using PCA\n",
    "word_2dviz.pca_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA puts most words close together, which is not easy to see each word clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension reduction using t-SNE\n",
    "word_2dviz.tsne_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE puts the words further away. For some words that are close together, such as \"walk\" and \"wonder\", it makes a lot of sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension reduction using UMAP\n",
    "word_2dviz.umap_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to t-SNE, UMAP puts words even further away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
